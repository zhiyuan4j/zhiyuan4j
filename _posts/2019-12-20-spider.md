---
layout: post
title:  爬虫经验
excerpt: "总结总结一些爬虫相关的经验"
categories: [experience,spider]
commnet: true
---


网上大火的都是python搞爬虫，我一直从事的是java爬虫，用的还是webmagic，对于目前的我够用，没有明显不爽的地方。

- 分析目标网站时，要以sources里的代码为准，而不是elements  
后端爬虫，通过请求/响应这种方式(HttpClient发个请求，区别于模拟浏览器里操作)，在分析目标网站的dom结构时(F12 打开浏览器的控制台 检查页面元素时)，要以sources里的数据为准，elements里的结构不一定就是sources里的请求返回的样子

- 浏览器插件，提升你的效率  
	比如chrome里的 `XPath Helper`

- 数据持久化的注意事项
	1. 注意数据库字段的最大值，避免data too long的异常出现
	2. 字符集：utf8 字符集是无法保存网页里的emoji的，utf8mb4可以保存。
	3. 原封不动的存下请求的返回值，可以帮助你找问题，当然这些字段不要加在业务表里，搞个爬虫的附表

- 策略问题
	1. 事实证明，热门网站，你访问的再慢，还是会被验证码
	2. 热门大站封你，就是给你返回个验证码页面；行业小站封你，大多数情况就直接给你封ip了
	3. UA的伪装肯定要有
	4. 大多数验证码都可以分析js什么的破掉，但是真的很费劲，而且对面换验证码的成本太低，某8验证码隔三岔五给你整个新的，还是找个代理ip的提供商靠谱点
	5. 代理不氪金的话，爬些免费的代理回来，不停的试，也能找到几个能用的(可用率是真的低，不知道他们写的存活xx天，可用什么的是test的哪个网址，baidu?)
	6. 代理网站也是网站，到他们上面白嫖代理ip时也要注意频次，封了就不能白嫖了..
	7. 推荐**优先爬取高价值页面**，免得列表页爬了一堆，然后被封了，一个详情页都没访问到，都在队列里躺着呢...

- 日志
	1. 建议一个爬虫输出到一个单独的日志文件中，当然，可以有一个收集error级别的总的日志文件。
	2. 单独的日志文件，就会限制你不太好使用模板方法模式，把多个爬虫的逻辑统一成一个模板方法。
	

- 用一个大的try,catch把自己的全部代码包起来，是个不错的选择  
	虽然各种编程规范里禁止你这么做，他们要求你try,catch的越小越好。  
	爬虫代码的输入是五花八门的，这就导致了运行时，肯定会有你意想不到的异常，不可避免的，不能因为某一些奇奇怪怪的页面导致你真个爬虫停止运行。

- 面向监狱编程？  
	这个词是，携程2019技术峰会，主持人在大数据专场最后结束语时蹦出来的。  
	如果你公司就是那你爬回的数据，简单处理下，然后卖信息，卖数据的，我是不太敢去这类公司的。面向监狱编程的风险比较高。  
	爬些数据，自己消费自己用，我觉得问题不大。  
	如有爬虫开发需求，跟公司签个补充协议保护下自己？